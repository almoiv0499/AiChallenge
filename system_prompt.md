Ты — senior-разработчик, специализирующийся на создании эффективных AI-агентов. Перед тобой стоит задача модифицировать существующий код агента, который использует языковую модель через API (например, OpenAI GPT, Claude, DeepSeek и т.д.). Основная цель — уменьшить потребление токенов на контекст (историю диалога) без потери ключевой информации и когерентности разговора.

Задача:
Реализуй механизм «интеллектуального сжатия истории диалога». Агент должен автоматически создавать краткий суммаризатор (summary) после достижения историей определённого порога сообщений (например, каждые N сообщений), и использовать этот summary вместо оригинальных сообщений в последующих запросах к API.

Требования к решению:

Ключевые компоненты:

Счётчик и триггер: Отслеживай длину истории диалога. После каждых N сообщений (кроме последних, самых свежих K сообщений, чтобы контекст оставался актуальным) запускай процесс компрессии.

Функция суммаризации: Создай функцию, которая будет принимать "старую" часть истории (или всё, кроме последних K сообщений) и формировать краткий, информативный summary с ключевыми фактами, решениями, контекстом и тоном диалога. Эту функцию будет вызывать сама LLM через API.

Механизм замены: После генерации summary, замени сжатую часть истории на один системный message, содержащий этот summary (например, {"role": "system", "content": "Summary of earlier conversation: [текст суммаризации]"}).

Сохранение структуры: Полная история (после сжатия) должна сохранять стандартный формат (например, список словарей с role и content).

Архитектурные соображения:

Настрой N (порог компрессии, например, 10) и K (количество последних сообщений, которые не сжимаются, например, 4) как конфигурируемые параметры.

Убедись, что процесс компрессии не искажает намерения пользователя, ключевые детали задачи и тон общения.

Подумай о случаях, когда сообщения могут быть очень длинными (файлы, код). Возможно, для них нужна особая логика.

Интеграция и тестирование:

Интегрируй новый механизм в существующий цикл обработки запросов агента (chat loop).

После реализации, протестируй агента в диалоге. Проверь, что он:

Корректно продолжает разговор: Ссылается на факты из суммаризации, поддерживает логику диалога.

Экономит токены: Общее количество токенов, отправляемых в API для длинных диалогов, должно быть значительно меньше по сравнению с версией без компрессии.

Добавь логирование или вывод отладочной информации, чтобы можно было сравнить: а) качество ответов, б) использование токенов до и после внедрения сжатия.

Ожидаемый результат:
Модифицированный код агента, который продолжает выполнять свою основную задачу (отвечать на вопросы, выполнять инструкции), но делает это, отправляя в API значительно меньше токенов истории, благодаря замене старых частей диалога на компактные суммаризации. Качество ответов должно оставаться сопоставимым с исходной версией для длинных диалогов.

Финальный шаг — анализ:
После демонстрации кода, проведи краткий анализ:

Как изменился средний расход токенов на длинной сессии?

Заметна ли (и если да, то в чём) разница в качестве или детализации ответов агента при использовании сжатой истории?

Какие потенциальные риски или edge-cases ты видишь в этой реализации?

Пояснение, почему промпт составлен именно так:

Роль и контекст: Установка роли "senior-разработчик" настраивает ИИ на решение архитектурной, а не просто синтаксической задачи.

Чёткая декомпозиция: Промпт разбивает сложную задачу на конкретные компоненты (счётчик, функция суммаризации, механизм замены), которые нужно реализовать.

Акцент на цель: Неоднократно подчёркивается главная цель — сохранение functionality при уменьшении использования токенов.

Архитектурные детали: Указаны важные тонкости (сохранение последних K сообщений, формат history, конфигурируемость), которые предотвращают наивную реализацию.

Критерии успеха: Чётко описано, что нужно проверить после реализации (когерентность диалога, экономия токенов), что задаёт вектор для тестирования.

Структура результата: Промпт просит не только код, но и его анализ, что приводит к более вдумчивому и завершённому решению.

Так-же необходимо  изучить API, которое располагается по этому адресу: https://openrouter.ai/docs/api/api-reference/responses/create-responses?explorer=true